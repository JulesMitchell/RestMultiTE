{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Working title]\n",
    "\n",
    "Authors: Mitchell, J. S., AnijÃ¤rv, T-E., Can, Adem., Hermens, D.F., & Lagopoulos, J.\n",
    "\n",
    "Code created by Jules Mitchell January 2024.\n",
    "\n",
    "You are free to use this or any other code from this repository for your own projects and publications. Citation or reference to the repository is not required, but would be much appreciated (see more on README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import networkx as nx\n",
    "from idtxl import idtxl_io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Set directory\n",
    "root_dir = os.getcwd()\n",
    "derivatives_folder = \"c:\\\\Users\\\\Bonnie\\\\OneDrive\\\\Documents\\\\GitHub\\\\IDTxl\\\\demos\\\\data\\\\derivatives\"\n",
    "results_folder = \"c:\\\\Users\\\\Bonnie\\\\OneDrive\\\\Documents\\\\GitHub\\\\IDTxl\\\\demos\\\\data\\\\derivatives\\\\results\"\n",
    "\n",
    "# Specify lists for conditional statements\n",
    "responders = [\"sub-01\", \"sub-02\"]\n",
    "\n",
    "# Electrode names\n",
    "electrode_names = pd.read_csv(\"C:/Users/Bonnie/OneDrive/Documents/GitHub/IDTxl/demos/data/sub-01/ses-01/EO_chan_activity.csv\", header=None, usecols=[0])\n",
    "electrode_names = list(electrode_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "# Mock Data\n",
    "def generate_adjacency_matrix(Size, N_cells): # Generate a single NxN adjacency matrix with N random integer values\n",
    "    matrix = np.zeros((Size, Size))\n",
    "    indices = np.random.choice(Size*Size, N_cells, replace=False)\n",
    "    matrix.flat[indices] = np.random.randint(1, 9, N_cells)\n",
    "    return matrix\n",
    "\n",
    "def generate_directed_graph(N_edges): # generate a single directed graph with N weighted edges\n",
    "    G = nx.DiGraph() \n",
    "    edges_to_add = np.random.choice(range(6), size=(N_edges, 2), replace=False)\n",
    "    for edge in edges_to_add:\n",
    "        i, j = edge\n",
    "        weight = np.random.uniform(0.1, 0.9)\n",
    "        G.add_edge(i, j, weight=weight)\n",
    "    return G\n",
    "\n",
    "def generate_mock_graphs(option, num_graphs, nodes, prob): # Generate mock graphs with various models\n",
    "    for i in range(num_graphs):\n",
    "        if option == 1:\n",
    "            # Erdos-Renyi graph\n",
    "            graph = nx.erdos_renyi_graph(nodes, prob)\n",
    "        elif option == 2:\n",
    "            # Barabasi-Albert graph\n",
    "            graph = nx.barabasi_albert_graph(nodes,2)\n",
    "        elif option == 3:\n",
    "            # Watts-Strogatz graph\n",
    "            graph = nx.watts_strogatz_graph(nodes, 2, prob)\n",
    "        elif option == 4:\n",
    "            # Directed Erdos-Renyi graph\n",
    "            graph = nx.fast_gnp_random_graph(nodes, prob, directed=True)\n",
    "        else:\n",
    "            # Default: Erdos-Renyi graph\n",
    "            graph = nx.erdos_renyi_graph(nodes, 0.2)\n",
    "    return graph\n",
    "\n",
    "# Graph Manipulation \n",
    "def combine_graphs(all_graphs, normalise=False):\n",
    "    average_graph = nx.DiGraph()\n",
    "    \n",
    "    N = len(all_graphs) if normalise else 1\n",
    "\n",
    "    for graph in all_graphs: # may need to enumerate?\n",
    "        for edge in graph.edges:\n",
    "            source, target = edge\n",
    "            if average_graph.has_edge(source, target):\n",
    "                average_graph[source][target]['weight'] += graph[source][target]['weight']\n",
    "            else:\n",
    "                # If the edge doesn't exist in the average_graph, add it\n",
    "                average_graph.add_edge(source, target, weight=graph[source][target]['weight'])\n",
    "    \n",
    "    # Normalize the edge weights after the loop\n",
    "    for edge in average_graph.edges:\n",
    "        source, target = edge\n",
    "        average_graph[source][target]['weight'] /= N\n",
    "\n",
    "    return average_graph\n",
    "\n",
    "# Plotting\n",
    "def graph_plot(graph, with_labels=False, with_weights=False):\n",
    "    # Settings\n",
    "    pos = nx.spring_layout(graph)\n",
    "    with_labels = True if with_labels else False\n",
    "    \n",
    "    if not with_weights:\n",
    "        edge_weights = 1.0\n",
    "    else:\n",
    "        edge_weights = [graph[edge[0]][edge[1]].get('weight', 1.0) for edge in graph.edges]\n",
    "\n",
    "    # Drawing the graph with varying line thickness based on edge weights\n",
    "    nx.draw_networkx(\n",
    "        graph, \n",
    "        pos, \n",
    "        with_labels=with_labels, \n",
    "        node_size=700, \n",
    "        node_color='skyblue', \n",
    "        font_size=8, \n",
    "        font_color='black', \n",
    "        font_weight='bold', \n",
    "        edge_color='gray', \n",
    "        width=edge_weights,  # Set the edge thickness based on edge weights\n",
    "        edge_cmap=plt.cm.Blues  # You can choose a colormap for the edges\n",
    "    )\n",
    "\n",
    "    plt.title('Graph Plot')\n",
    "    plt.show()\n",
    "\n",
    "# Load IDTxL results\n",
    "def load_graphs(folder, responders): # change to load_bids\n",
    "    # Initialize an empty list to store graphs\n",
    "    all_results = []\n",
    "\n",
    "    # Loop through subject folders\n",
    "    for subject_folder in os.listdir(folder):\n",
    "        subject_path = os.path.join(folder, subject_folder)\n",
    "        \n",
    "        # Check if the subject is a responder or non-responder\n",
    "        if subject_folder in responders:\n",
    "            response_category = \"responder\"\n",
    "        else:\n",
    "            response_category = \"non_responder\"\n",
    "\n",
    "        # Loop through session folders\n",
    "        for session_folder in os.listdir(subject_path):\n",
    "            session_path = os.path.join(subject_path, session_folder)\n",
    "\n",
    "            # Assuming graph files are stored as text files (modify as needed)\n",
    "            results = [f for f in os.listdir(session_path) if f.endswith(\".p\")] # Adjust file type as required\n",
    "\n",
    "            # Load each graph file into a directed graph\n",
    "            for result in results:\n",
    "                graph_file_path = os.path.join(session_path, result)\n",
    "\n",
    "                # Load results class from file (modify based on your data format)\n",
    "                idtxl_file = pickle.load(open(graph_file_path, 'rb'))\n",
    "\n",
    "                # Convert to networkx graph    \n",
    "                weights = 'te'\n",
    "                adj_matrix = idtxl_file.get_adjacency_matrix(weights=weights, fdr=False, weight_type=float)\n",
    "                network = io.export_networkx_graph(adjacency_matrix=adj_matrix, weights=weights)\n",
    "                \n",
    "                # Determine task type\n",
    "                if 'EC' in graph_file_path:\n",
    "                    task = 'EC'\n",
    "                else:\n",
    "                    task = 'EO'\n",
    "\n",
    "                # Add the information to the list\n",
    "                all_results.append({\n",
    "                    \"subject\": subject_folder,\n",
    "                    \"response_category\": response_category,\n",
    "                    \"timepoint\": session_folder,\n",
    "                    \"task\": task,\n",
    "                    \"results\": idtxl_file,\n",
    "                    \"network\": network,\n",
    "                    \"adj_matrix\": adj_matrix,\n",
    "                    'weights': weights\n",
    "                })\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Global Network Measures\n",
    "def calculate_global_network_measures(all_graphs):\n",
    "    # Create an empty list to store data\n",
    "    data = []\n",
    "\n",
    "    # Loop through each loaded graph\n",
    "    for graph_info in all_graphs:\n",
    "        subject = graph_info[\"subject\"]\n",
    "        response_category = graph_info[\"response_category\"]\n",
    "        timepoint = graph_info[\"timepoint\"]\n",
    "        task = graph_info[\"task\"]\n",
    "        network = graph_info[\"network\"]\n",
    "        undirected_network = network.to_undirected() # Need to change each network to undirected to calculate small-world network and global efficiency\n",
    "\n",
    "        # Global measures\n",
    "        global_efficiency = nx.global_efficiency(undirected_network) # How efficiently information is exchanged (higher values indicate higher integration)\n",
    "        #smallworld = nx.sigma(undirected_network)  # Balance of local clustering and shortest path length\n",
    "\n",
    "        # Append data to the list\n",
    "        data.append({\n",
    "            \"subject\": subject,\n",
    "            \"response_category\": response_category,\n",
    "            \"timepoint\": timepoint,\n",
    "            \"task\": task,\n",
    "            \"GEf\": global_efficiency\n",
    "            #\"SM\": smallworld\n",
    "            })\n",
    "   \n",
    "    # Create a DataFrame from the list of data\n",
    "    global_df = pd.DataFrame(data)\n",
    "\n",
    "    return global_df\n",
    "\n",
    "# Local Network Measures\n",
    "def calculate_local_network_measures(all_graphs, electrode_names):\n",
    "    # Create an empty list to store data\n",
    "    clustering_temp = []\n",
    "    betweenness_temp = []\n",
    "    eigen_temp = []\n",
    "    indegree_temp = []\n",
    "    outdegree_temp = []\n",
    "\n",
    "    # Loop through each loaded graph\n",
    "    for graph_info in all_graphs:\n",
    "        subject = graph_info[\"subject\"]\n",
    "        response_category = graph_info[\"response_category\"]\n",
    "        timepoint = graph_info[\"timepoint\"]\n",
    "        task = graph_info[\"task\"]\n",
    "        network = graph_info[\"network\"]\n",
    "\n",
    "        # Local measures\n",
    "        clustering = nx.clustering(network)\n",
    "        betweenness = nx.betweenness_centrality(network)\n",
    "        # eigen = nx.eigenvector_centrality(network)\n",
    "        indegree = nx.in_degree_centrality(network)\n",
    "        outdegree = nx.out_degree_centrality(network)\n",
    "        \n",
    "        # Rename columns with electrode names (replace this with your electrode names)\n",
    "        clustering_renamed = {electrode_names[i]: clustering.get(i, 0) for i in range(len(electrode_names))}\n",
    "        betweenness_renamed = {electrode_names[i]: betweenness.get(i, 0) for i in range(len(electrode_names))}\n",
    "        # eigen_renamed = {electrode_names[i]: eigen.get(i, 0) for i in range(len(electrode_names))}\n",
    "        indegree_renamed = {electrode_names[i]: indegree.get(i, 0) for i in range(len(electrode_names))}\n",
    "        outdegree_renamed = {electrode_names[i]: outdegree.get(i, 0) for i in range(len(electrode_names))}\n",
    "\n",
    "\n",
    "        # Append data to the lists\n",
    "        clustering_temp.append({\n",
    "            \"subject\": subject,\n",
    "            \"response_category\": response_category,\n",
    "            \"session\": timepoint,\n",
    "            \"task\": task,\n",
    "            \"measure\": \"ClCoef\",\n",
    "            ** clustering_renamed\n",
    "            }) \n",
    "         \n",
    "        betweenness_temp.append({\n",
    "            \"subject\": subject,\n",
    "            \"response_category\": response_category,\n",
    "            \"session\": timepoint,\n",
    "            \"task\": task,\n",
    "            \"measure\": \"Btwn\",\n",
    "            ** betweenness_renamed\n",
    "            })  \n",
    "        \n",
    "        #eigen_temp.append({\n",
    "            #\"subject\": subject,\n",
    "            #\"response_category\": response_category,\n",
    "            #\"session\": session,\n",
    "            #\"task\": task,\n",
    "            #** eigen_renamed\n",
    "            #})  \n",
    "        \n",
    "        indegree_temp.append({\n",
    "            \"subject\": subject,\n",
    "            \"response_category\": response_category,\n",
    "            \"session\": timepoint,\n",
    "            \"task\": task,\n",
    "            \"measure\": \"InDgr\",\n",
    "            ** indegree_renamed\n",
    "            })  \n",
    "        \n",
    "        outdegree_temp.append({\n",
    "            \"subject\": subject,\n",
    "            \"response_category\": response_category,\n",
    "            \"session\": timepoint,\n",
    "            \"task\": task,\n",
    "            \"measure\": \"OutDgr\",\n",
    "            ** outdegree_renamed\n",
    "            })  \n",
    "        \n",
    "    # Create a DataFrame from the list of data\n",
    "    clustering_df = pd.DataFrame(clustering_temp)\n",
    "    betweenness_df = pd.DataFrame(betweenness_temp)\n",
    "    #eigen_df = pd.DataFrame(eigen_temp)\n",
    "    indegree_df = pd.DataFrame(indegree_temp)\n",
    "    outdegree_df = pd.DataFrame(outdegree_temp)\n",
    "\n",
    "    # Combine dataframes\n",
    "    local_df = pd.concat([clustering_df, betweenness_df, indegree_df, outdegree_df], axis=0)\n",
    "\n",
    "    # Set 'subject' column as the index\n",
    "    local_df.set_index('subject', inplace=True)\n",
    "\n",
    "    # Sort the DataFrame by the index\n",
    "    local_df.sort_index(inplace=True)\n",
    "\n",
    "    return local_df \n",
    "\n",
    "def te_extraction (all_graphs):\n",
    "\n",
    "    # Initialize temporary variables\n",
    "    subject_values = []\n",
    "    task_values = []\n",
    "    timepoint_values = []\n",
    "    caudal_left_values = []\n",
    "    rostral_left_values = []\n",
    "    caudal_right_values = []\n",
    "    rostral_right_values = []\n",
    "    asymmetry_left_values = []\n",
    "    asymmetry_right_values = []\n",
    "\n",
    "    # Iterate through each participant-file in the list\n",
    "    for participant in range(len(all_graphs)):\n",
    "\n",
    "        # Initialise TE placeholders\n",
    "        caudal_left_sum = 0\n",
    "        caudal_right_sum = 0\n",
    "        rostral_left_sum = 0\n",
    "        rostral_right_sum = 0\n",
    "        \n",
    "        # Load participant details\n",
    "        subject = all_graphs[participant]['subject']\n",
    "        task = all_graphs[participant]['task']\n",
    "        timepoint = all_graphs[participant]['timepoint']\n",
    "        \n",
    "        # Load the channel weights list per participant-file\n",
    "        channel_weights_list = all_graphs[participant]['adj_matrix'].get_edge_list()\n",
    "        \n",
    "        # Iterate through the list and apply the conditional statement\n",
    "        for i, j, weight in channel_weights_list:\n",
    "\n",
    "            # Left hemisphere frontal to parietal: 0 (Fp1), 1 (AF3), 2 (F7), 3 (F3) --> 8 (CP1), 9 (CP5), 10 (P7), 11 (P3) \n",
    "            if (i == 0 or i == 1 or i == 2 or i == 3) and (j == 8 or j == 9 or j == 10 or j == 11):\n",
    "                caudal_left_sum += weight\n",
    "\n",
    "            # Right hemisphere frontal to parietal: 29 (Fp2), 28 (AF4), 27 (F8), 26 (F4) --> 21 (CP2), 20 (CP6), 19 (P8), 18 (P4) \n",
    "            if (i == 29 or i == 28 or i == 27 or i == 26) and (j == 21 or j == 20 or j == 19 or j == 18):\n",
    "                caudal_right_sum += weight\n",
    "\n",
    "            # Left hemisphere parietal to frontal: 8 (CP1), 9 (CP5), 10 (P7), 11 (P3) --> 0 (Fp1), 1 (AF3), 2 (F7), 3 (F3)\n",
    "            if (i == 8 or i == 9 or i == 10 or i == 11) and (j == 0 or j == 1 or j == 2 or j == 3):\n",
    "                rostral_left_sum += weight\n",
    "\n",
    "            # Right hemisphere parietal to frontal: 21 (CP2), 20 (CP6), 19 (P8), 18 (P4) --> 29 (Fp2), 28 (AF4), 27 (F8), 26 (F4)\n",
    "            if (i == 21 or i == 20 or i == 19 or i == 18) and (j == 29 or j == 28 or j == 27 or j == 26):\n",
    "                rostral_right_sum += weight\n",
    "\n",
    "        # Calculate the final values by scaling by the 1/number of nodes\n",
    "        caudal_left_final_value = caudal_left_sum * (1/8)\n",
    "        caudal_right_final_value = caudal_right_sum * (1/8)\n",
    "        rostral_left_final_value = rostral_left_sum * (1/8)\n",
    "        rostral_right_final_value = rostral_right_sum * (1/8)\n",
    "\n",
    "        # Calculate asymmetry value\n",
    "        if (caudal_left_final_value + rostral_left_final_value) == 0: \n",
    "            asymmetry_left = 0\n",
    "        else:\n",
    "            asymmetry_left = (caudal_left_final_value - rostral_left_final_value)/ (caudal_left_final_value + rostral_left_final_value)\n",
    "\n",
    "        if (caudal_right_final_value + rostral_right_final_value) == 0: \n",
    "            asymmetry_right = 0\n",
    "        else:\n",
    "            asymmetry_right = (caudal_right_final_value - rostral_right_final_value)/ (caudal_right_final_value + rostral_right_final_value)\n",
    "\n",
    "        # Append the values\n",
    "        subject_values.append(subject)\n",
    "        task_values.append(task)\n",
    "        timepoint_values.append(timepoint)\n",
    "        caudal_left_values.append(caudal_left_final_value)\n",
    "        caudal_right_values.append(caudal_right_final_value)\n",
    "        rostral_left_values.append(rostral_left_final_value)\n",
    "        rostral_right_values.append(rostral_right_final_value)\n",
    "        asymmetry_left_values.append(asymmetry_left)\n",
    "        asymmetry_right_values.append(asymmetry_right)\n",
    "\n",
    "        # Save the results to a DataFrame with an ID column\n",
    "        te_extracted_df = pd.DataFrame({\n",
    "            'Subject': subject_values,\n",
    "            'Task': task_values,\n",
    "            'Timepoint': timepoint_values, # add subject ID, task, timepoint\n",
    "            'Caudal_Left': caudal_left_values,\n",
    "            'Caudal_Right': caudal_right_values,\n",
    "            'Rostral_Left': rostral_left_values,\n",
    "            'Rostral_Right': rostral_right_values,\n",
    "            'Asymmetry_Left': asymmetry_left_values,\n",
    "            'Asymmetry_Right': asymmetry_right_values\n",
    "            })\n",
    "\n",
    "    return te_extracted_df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock Data Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate Mock data (Adjancency Matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify size (N-by-N)\n",
    "Size = 4\n",
    "\n",
    "# Specify the number of cells you want to populate in each matrix (replace N_cells with your desired number)\n",
    "N_cells = int(Size/2)\n",
    "\n",
    "# Specify the number of matrices you want (replace N_matrices with your desired number)\n",
    "N_matrices = 2\n",
    "\n",
    "# Generate N_matrices adjacency matrices\n",
    "adjacency_matrices = [generate_adjacency_matrix(Size, N_cells) for _ in range(N_matrices)]\n",
    "\n",
    "# Print the generated adjacency matrices\n",
    "for i, matrix in enumerate(adjacency_matrices):\n",
    "    print(f\"Adjacency Matrix {i + 1}:\\n{matrix}\\n\")\n",
    "\n",
    "# Print the resulting average adjacency matrix\n",
    "print(\"Average Adjacency Matrix:\")\n",
    "print(np.mean(adjacency_matrices, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Mock Data (Directed Graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of edges you want to populate in each graph (replace N_edges with your desired number)\n",
    "N_edges = 2\n",
    "\n",
    "# Specify the number of directed graphs you want (replace N_graphs with your desired number)\n",
    "N_graphs = 2\n",
    "\n",
    "# Generate N_graphs directed graphs\n",
    "graphs = [generate_directed_graph(N_edges) for _ in range(N_graphs)]\n",
    "\n",
    "# Print the generated graphs\n",
    "for i, graph in enumerate(graphs):\n",
    "    print(f\"Directed Graph {i + 1}:\\nNodes: {graph.nodes}\\nEdges: {graph.edges(data=True)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Mock Data (Graph Options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional mock graphs\n",
    "graph = generate_mock_graphs(option=4, num_graphs=1, nodes=10, prob = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Transfer Entropy Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files (Note, there is currently an issue with the gml file load (something to do with strings))\n",
    "all_graphs = load_graphs(derivatives_folder, responders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify results between single target and adj_matrix edge list\n",
    "all_graphs[1]['results'].get_single_target(0, fdr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_graphs[1]['adj_matrix'].get_edge_list() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TE Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TE values for significant source-target pairs and calculate information flow metrics\n",
    "te_data = te_extraction(all_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results as csv\n",
    "os.chdir(results_folder)\n",
    "te_data.to_csv('OKTOS_teMeasures.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topographical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global measures\n",
    "global_data = calculate_global_network_measures(all_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local measures\n",
    "local_data = calculate_local_network_measures(all_graphs, electrode_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results as csv\n",
    "os.chdir(results_folder)\n",
    "global_data.to_csv('OKTOS_GlobalMeasures.csv', index=False)\n",
    "local_data.to_csv('OKTOS_LocalMeasures.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Graphs/Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine directed graphs for responders and non-responders across timepoint and task\n",
    "average_graph = combine_graphs(graphs, normalise = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
