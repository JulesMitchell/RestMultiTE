{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resting State Transfer Entropy Differentiates Responders and Non-Responders to Oral Ketamine Treatment for Suicidality\n",
    "\n",
    "Authors: Mitchell, J. S., Anij√§rv, T-E., Can, Adem., Hermens, D.F., & Lagopoulos, J.\n",
    "\n",
    "Code created by Jules Mitchell March 2024.\n",
    "\n",
    "You are free to use this or any other code from this repository for your own projects and publications. Citation or reference to the repository is not required, but would be much appreciated (see more on README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import os \n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Import classes\n",
    "from idtxl.multivariate_te import MultivariateTE\n",
    "from idtxl.data import Data\n",
    "from idtxl.visualise_graph import plot_network, plot_selected_vars, plot_network_comparison\n",
    "from idtxl.stats import network_fdr\n",
    "from idtxl import idtxl_io as io\n",
    "\n",
    "## Choose estimator to import\n",
    "# import idtxl.estimators_python # Non-linear continous data Kraskov estimator \n",
    "import idtxl.estimators_jidt\n",
    "\n",
    "# Set directories\n",
    "os.chdir('C:/Users/j_m289/Pictures/GitHub/Studies/OKTOS/RS_TransferEntropy/data')\n",
    "root_dir = os.getcwd()\n",
    "output_dir = '/derivatives'\n",
    "\n",
    "# Define responders\n",
    "responders = [\"sub-01\"]\n",
    "\n",
    "# Initialise analysis object and define settings\n",
    "network_analysis = MultivariateTE()\n",
    "settings = {'cmi_estimator': 'JidtGaussianCMI', # Note, gaussian is faster but finds spurious connections in a non-linear system OpenCLKraskovCMI/JidtGaussianCMI/JidtKraskovCMI\n",
    "            #'local_values': True,\n",
    "            'max_lag_sources': 5,\n",
    "            'min_lag_sources': 1,\n",
    "            'n_perm_max_stat': 500,\n",
    "            'alpha_max_stat': 0.05,\n",
    "            'n_perm_min_stat': 500,\n",
    "            'alpha_min_stat': 0.05,\n",
    "            'permute_in_time': True,\n",
    "            'perm_type': 'random',\n",
    "            'n_perm_omnibus': 500,\n",
    "            'alpha_omnibus': 0.05,\n",
    "            'n_perm_max_seq': 500,\n",
    "            'alpha_max_seq': 0.05\n",
    "            #'fdr_correction': True, \n",
    "            #'alpha_fdr': 0.05,\n",
    "            #'correct_by_target': True\n",
    "            #'tau_sources': 2, # Subset of past samples to include (every ith sample)\n",
    "            #'tau_target': 2, # Subset of past samples to include (every ith sample)\n",
    "            # 'add_conditionals': 'faes' # adds the current value of all sources to the conditioning set (minimises volume conduction effects)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "def load_bids(bids_folder, classifier): # change to load_bids\n",
    "    # Initialize an empty list to store graphs\n",
    "    bids_files = []\n",
    "\n",
    "    # Loop through subject folders\n",
    "    for subject_folder in os.listdir(bids_folder):\n",
    "        subject_path = os.path.join(bids_folder, subject_folder)\n",
    "        \n",
    "        # Check if the subject is a responder or non-responder\n",
    "        if subject_folder in classifier: # change this to a key based system\n",
    "            response_category = \"responder\"\n",
    "        else:\n",
    "            response_category = \"non_responder\"\n",
    "\n",
    "        # Loop through session folders\n",
    "        for session_folder in os.listdir(subject_path):\n",
    "            session_path = os.path.join(subject_path, session_folder)\n",
    "\n",
    "            # Assuming graph files are stored as text files (modify as needed)\n",
    "            files = [f for f in os.listdir(session_path) if f.endswith(\".csv\")] # Adjust file type as required\n",
    "\n",
    "            # Load each graph file into a directed graph\n",
    "            for file in files:\n",
    "                file_path = os.path.join(session_path, file)\n",
    "\n",
    "                # Load graph from file (modify based on your data format)\n",
    "                df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "                # Drop channels from dataframe and select portion of resting state data\n",
    "                df = df.drop(columns = 0)\n",
    "                #df = df.iloc[:, 0:100] # Here is where I want the 'cut' parameter\n",
    "\n",
    "                # Save dataframe as IDTxl data object and specify dimensions\n",
    "                data = Data(df, dim_order='ps')\n",
    "\n",
    "                if 'EC' in file_path:\n",
    "                    task = 'EC'\n",
    "                else:\n",
    "                    task = 'EO'\n",
    "\n",
    "                # Add the graph information to the list\n",
    "                bids_files.append({\n",
    "                    \"subject\": subject_folder,\n",
    "                    \"response_category\": response_category,\n",
    "                    \"session\": session_folder,\n",
    "                    \"task\": task,\n",
    "                    \"data\": data\n",
    "                })\n",
    "\n",
    "    return bids_files \n",
    "\n",
    "def combine_targets(folder, threshold = 2): # change to load_bids\n",
    "\n",
    "    # Loop through subject folders\n",
    "    for subject_folder in os.listdir(folder):\n",
    "        subject_path = os.path.join(folder, subject_folder)\n",
    "\n",
    "        # Loop through session folders\n",
    "        for session_folder in os.listdir(subject_path):\n",
    "            session_path = os.path.join(subject_path, session_folder)\n",
    "\n",
    "            # Assuming graph files are stored as text files (modify as needed)\n",
    "            files = [f for f in os.listdir(session_path) if f.endswith(\".p\")] # Adjust file type as required\n",
    "\n",
    "            # Load results using pickle\n",
    "            res_list = []\n",
    "\n",
    "            for file in files:\n",
    "                file_path = os.path.join(session_path, file)\n",
    "                res_list.append(pickle.load(open(file_path, 'rb')))\n",
    "\n",
    "            res = network_fdr({'alpha_fdr': 0.05, 'fdr_constant': threshold}, *res_list)\n",
    "\n",
    "            # Construct the output file path\n",
    "            output_dir = os.path.join('derivatives', subject_folder, session_folder)\n",
    "            os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "            file_path = os.path.join(output_dir, f'{subject_folder}_{session_folder}_EO_network.p')\n",
    "            \n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(res, f)\n",
    "\n",
    "            print('Successfully Combined Targets for {}/{}'.format(str(subject_folder), str(session_folder)))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mock dataset if required\n",
    "data = Data ()\n",
    "data.generate_mute_data(n_samples=1000, n_replications=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load participant files from BIDS structure\n",
    "bids_files = load_bids(root_dir, responders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network analysis\n",
    "for x in range(len(bids_files)):\n",
    "    temp_data = bids_files[x]['data']\n",
    "    # mTE analysis\n",
    "    results = network_analysis.analyse_network(settings=settings, data=temp_data)\n",
    "\n",
    "    # Get the subject and session information\n",
    "    subject = bids_files[x]['subject']\n",
    "    session = bids_files[x]['session']\n",
    "\n",
    "    # Construct the file path\n",
    "    output_dir = os.path.join('derivatives', subject, session)\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "    file_path = os.path.join(output_dir, f'{subject}_{session}_EO_network.p')\n",
    "\n",
    "    # File save\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single target analysis\n",
    "targets = range(1)\n",
    "\n",
    "# Run mTE using IDTxl - current issue: Full file path not specified properly\n",
    "for x in range(len(bids_files)):\n",
    "    for t in targets:\n",
    "        start_time = time.time()  # Record the start time\n",
    "\n",
    "        temp_data = bids_files[x]['data']\n",
    "        results = network_analysis.analyse_single_target(settings=settings, data=temp_data, target=t)\n",
    "\n",
    "        # Get the subject and session information\n",
    "        subject = bids_files[x]['subject']\n",
    "        session = bids_files[x]['session']\n",
    "\n",
    "        # Construct the file path\n",
    "        output_dir = os.path.join('derivatives', subject, session)\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "        file_path = os.path.join(output_dir, f'{subject}_{session}_{t}_EO_target.p')\n",
    "\n",
    "        # File save\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "\n",
    "        end_time = time.time()  # Record the end time\n",
    "        elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "        print(f\"Time taken for {subject}, {session}, target {t}: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'C:/Users/j_m289/Pictures/GitHub/Studies/OKTOS/RS_TransferEntropy/data/derivatives/'\n",
    "combine_targets(folder, threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BrainNet Viewer - For group comparisons of connectivity with glass brains\n",
    "outfile = 'brain_net'\n",
    "n_nodes = results.data_properties.n_nodes\n",
    "mni_coord = np.random.randint(10, size=(n_nodes, 3))\n",
    "node_color = np.random.randint(10, size=n_nodes)\n",
    "node_size = np.random.randint(10, size=n_nodes)\n",
    "labels = list\n",
    "adj_matrix = results.get_adjacency_matrix(\n",
    "    weights='max_te_lag', fdr=False,)\n",
    "io.export_brain_net_viewer(adjacency_matrix=adj_matrix,\n",
    "                            mni_coord=mni_coord,\n",
    "                            file_name=outfile,\n",
    "                            labels=labels,\n",
    "                            node_color=node_color,\n",
    "                            node_size=node_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PBS Scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bash script \n",
    "network_size = 32\n",
    "\n",
    "# Define PBS script\n",
    "bash_lines = '\\n'.join([\n",
    "    '#! /bin/bash',\n",
    "    # set project name\n",
    "    '#PBS -P OKTOS_TE',\n",
    "    # set job name\n",
    "    '#PBS -N NetorkAnalysis',\n",
    "    # choose number of cores and memory\n",
    "    '#PBS -l select=1:ncpus=1:mem=1GB', # select is a multiplier for ncpus and mem\n",
    "    # set walltime hh:mm:ss\n",
    "    '#PBS -l walltime=16:00:00',\n",
    "    # set job array numbers to match network size\n",
    "    '#PBS -J 0-{}'.format(network_size),\n",
    "    # load Python\n",
    "    'module load python/3.7.3',\n",
    "    # if necessary, activate local environment where IDTxl is installed\n",
    "    'source /ProjectName/idtxl_env/bin/activate',\n",
    "    # run analysis on single target\n",
    "    'python analyse_single_target.py $PBS_ARRAY_INDEX'\n",
    "    # output directory?\n",
    "    #'PBS -o myscript.out'\n",
    "    ])\n",
    "\n",
    "# Generate and save PBS script file\n",
    "bash_script_name = 'parallel_analysis_using_PBS_example.pbs'\n",
    "with open(bash_script_name, 'w', newline='\\n') as bash_file:\n",
    "    bash_file.writelines(bash_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idtxl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
