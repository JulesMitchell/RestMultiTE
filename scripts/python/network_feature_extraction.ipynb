{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resting state EEG-derived transfer entropy\n",
    "\n",
    "Code created by Jules Mitchell January 2024.\n",
    "\n",
    "You are free to use this or any other code from this repository for your own projects and publications. Citation or reference to the repository is not required, but would be much appreciated (see more on README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import os\n",
    "import networkx as nx\n",
    "import netrd as net\n",
    "import igraph as ig\n",
    "from idtxl import idtxl_io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define functions\n",
    "# Mock Data\n",
    "def generate_adjacency_matrix(Size, N_cells): # Generate a single NxN adjacency matrix with N random integer values\n",
    "    matrix = np.zeros((Size, Size))\n",
    "    indices = np.random.choice(Size*Size, N_cells, replace=False)\n",
    "    matrix.flat[indices] = np.random.randint(1, 9, N_cells)\n",
    "    return matrix\n",
    "\n",
    "def generate_directed_graph(N_edges): # generate a single directed graph with N weighted edges\n",
    "    G = nx.DiGraph() \n",
    "    edges_to_add = np.random.choice(range(6), size=(N_edges, 2), replace=False)\n",
    "    for edge in edges_to_add:\n",
    "        i, j = edge\n",
    "        weight = np.random.uniform(0.1, 0.9)\n",
    "        G.add_edge(i, j, weight=weight)\n",
    "    return G\n",
    "\n",
    "def generate_mock_graphs(option, num_graphs, nodes, prob): # Generate mock graphs with various models\n",
    "    for i in range(num_graphs):\n",
    "        if option == 1:\n",
    "            # Erdos-Renyi graph\n",
    "            graph = nx.erdos_renyi_graph(nodes, prob)\n",
    "        elif option == 2:\n",
    "            # Barabasi-Albert graph\n",
    "            graph = nx.barabasi_albert_graph(nodes,2)\n",
    "        elif option == 3:\n",
    "            # Watts-Strogatz graph\n",
    "            graph = nx.watts_strogatz_graph(nodes, 2, prob)\n",
    "        elif option == 4:\n",
    "            # Directed Erdos-Renyi graph\n",
    "            graph = nx.fast_gnp_random_graph(nodes, prob, directed=True)\n",
    "        else:\n",
    "            # Default: Erdos-Renyi graph\n",
    "            graph = nx.erdos_renyi_graph(nodes, 0.2)\n",
    "    return graph\n",
    "\n",
    "# Graph Manipulation \n",
    "def combine_graphs(all_graphs, normalise=False):\n",
    "    average_graph = nx.DiGraph()\n",
    "    \n",
    "    N = len(all_graphs) if normalise else 1\n",
    "\n",
    "    for graph in all_graphs: # may need to enumerate?\n",
    "        for edge in graph.edges:\n",
    "            source, target = edge\n",
    "            if average_graph.has_edge(source, target):\n",
    "                average_graph[source][target]['weight'] += graph[source][target]['weight']\n",
    "            else:\n",
    "                # If the edge doesn't exist in the average_graph, add it\n",
    "                average_graph.add_edge(source, target, weight=graph[source][target]['weight'])\n",
    "    \n",
    "    # Normalize the edge weights after the loop\n",
    "    for edge in average_graph.edges:\n",
    "        source, target = edge\n",
    "        average_graph[source][target]['weight'] /= N\n",
    "\n",
    "    return average_graph\n",
    "\n",
    "# Plotting\n",
    "def graph_plot(graph, with_labels=False, with_weights=False):\n",
    "    # Settings\n",
    "    pos = nx.spring_layout(graph)\n",
    "    with_labels = True if with_labels else False\n",
    "    \n",
    "    if not with_weights:\n",
    "        edge_weights = 1.0\n",
    "    else:\n",
    "        edge_weights = [graph[edge[0]][edge[1]].get('weight', 1.0) for edge in graph.edges]\n",
    "\n",
    "    # Drawing the graph with varying line thickness based on edge weights\n",
    "    nx.draw_networkx(\n",
    "        graph, \n",
    "        pos, \n",
    "        with_labels=with_labels, \n",
    "        node_size=700, \n",
    "        node_color='skyblue', \n",
    "        font_size=8, \n",
    "        font_color='black', \n",
    "        font_weight='bold', \n",
    "        edge_color='gray', \n",
    "        width=edge_weights,  # Set the edge thickness based on edge weights\n",
    "        edge_cmap=plt.cm.Blues  # You can choose a colormap for the edges\n",
    "    )\n",
    "\n",
    "    plt.title('Graph Plot')\n",
    "    plt.show()\n",
    "\n",
    "# Load IDTxL results\n",
    "def load_graphs(folder, include = None, weights = 'binary'):\n",
    "    \"\"\"\n",
    "    Load graph data from pickle files in specified folders.\n",
    "\n",
    "    Parameters:\n",
    "    - folder (str or Path): Path to the root folder containing subject and session folders.\n",
    "    - responders (list): List of subjects classified as responder.\n",
    "\n",
    "    Returns:\n",
    "    - all_results (list): List of dictionaries containing loaded graph data.\n",
    "    \"\"\"\n",
    "    folder = Path(folder)\n",
    "    all_results = []\n",
    "\n",
    "    # Loop through subject folders\n",
    "    for subject_folder in folder.iterdir():\n",
    "        if not subject_folder.is_dir():\n",
    "            continue\n",
    "\n",
    "        subject_name = subject_folder.name\n",
    "        if include is not None and subject_name not in include:\n",
    "            continue\n",
    "        \n",
    "        # Loop through session folders\n",
    "        for session_folder in subject_folder.iterdir():\n",
    "            if not session_folder.is_dir():\n",
    "                continue\n",
    "            \n",
    "            # Assuming graph files are stored as pickle files\n",
    "            results = list(session_folder.glob('*.p'))\n",
    "            \n",
    "            # Skip processing if there are no pickle files\n",
    "            if not results:\n",
    "                print(f\"No .p files found in {session_folder}\")\n",
    "                continue\n",
    "\n",
    "            # Load each graph file into a directed graph\n",
    "            for result_path in results:\n",
    "                try:\n",
    "                    with open(result_path, 'rb') as f:\n",
    "                        idtxl_file = pickle.load(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {result_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert to networkx graph\n",
    "                weights = weights\n",
    "                adj_matrix = idtxl_file.get_adjacency_matrix(weights=weights, fdr=False) #weight type \n",
    "                networkx = io.export_networkx_graph(adjacency_matrix=adj_matrix, weights=weights)\n",
    "                igraph = Graph.from_networkx(networkx)\n",
    "\n",
    "                # Determine condition type\n",
    "                if 'EC' in result_path.stem:\n",
    "                    task = 'EC'\n",
    "                else:\n",
    "                    task = 'EO'\n",
    "\n",
    "                # Add the information to the list\n",
    "                all_results.append({\n",
    "                    \"subject\": subject_folder.name,\n",
    "                    'timepoint': session_folder.name,\n",
    "                    \"condition\": task,\n",
    "                    \"results\": idtxl_file,\n",
    "                    \"networkx\": networkx,\n",
    "                    \"igraph\": igraph,\n",
    "                    \"adj_matrix\": adj_matrix,\n",
    "                    'weights': weights\n",
    "                })\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def load_classifier_file(file_path):\n",
    "    # Initialize an empty dictionary to store the loaded data\n",
    "    loaded_mapping = {}\n",
    "\n",
    "    # Open the file in read mode ('r')\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read each line in the file\n",
    "        for line in file:\n",
    "            # Split each line by the first occurrence of ':' to separate key and value\n",
    "            key, value = line.split(':', 1)\n",
    "            # Remove any leading/trailing whitespaces and convert value to list\n",
    "            loaded_mapping[key.strip()] = eval(value.strip())\n",
    "\n",
    "    return loaded_mapping\n",
    "\n",
    "def load_electrodes_file(file_path):\n",
    "    # Initialize an empty dictionary to store the loaded data\n",
    "    electrodes = []\n",
    "\n",
    "    # Open the file in read mode ('r')\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read each line in the file\n",
    "        for line in file:\n",
    "            # Strip any extra whitespace (like newline characters) and add to the list\n",
    "            electrodes.append(line.strip())\n",
    "\n",
    "    return electrodes\n",
    "    \n",
    "def add_classifier(bids, classifier, designation=[1, 0]):\n",
    "\n",
    "    for entry in bids:\n",
    "        participant = entry['subject']\n",
    "        timepoint = entry['timepoint']\n",
    "        \n",
    "        found_designation = designation[1]  # Default designation if not found\n",
    "        \n",
    "        # Check if the participant is in the response_status_mapping for the given timepoint\n",
    "        if timepoint in classifier:\n",
    "            if participant in classifier[timepoint]:\n",
    "                found_designation = designation[0]\n",
    "        \n",
    "        # Assign the found designation to the bid\n",
    "        entry['group'] = found_designation\n",
    "\n",
    "def calc_global_efficiency(G):\n",
    "    n = len(G.nodes())  # Number of nodes in the graph\n",
    "    total_efficiency = 0\n",
    "    \n",
    "    # Iterate over all pairs of nodes\n",
    "    for i in G.nodes():\n",
    "        lengths = nx.single_source_shortest_path_length(G, i)  # Shortest path lengths from node i\n",
    "        for j in G.nodes():\n",
    "            if i != j:\n",
    "                if j in lengths:  # Ensure there's a path from i to j\n",
    "                    total_efficiency += 1 / lengths[j]\n",
    "                else:\n",
    "                    total_efficiency += 0  # No path between i and j, contribute 0 efficiency\n",
    "    \n",
    "    # Normalize by n(n-1)\n",
    "    return total_efficiency / (n * (n - 1))\n",
    "\n",
    "# Global Network Measures\n",
    "def calculate_global_network_measures(all_graphs):\n",
    "    # Create an empty list to store data\n",
    "    data = []\n",
    "\n",
    "    # Loop through each loaded graph\n",
    "    for graph_info in all_graphs:\n",
    "        #igraph = graph_info[\"igraph\"]\n",
    "        network = graph_info[\"networkx\"]\n",
    "\n",
    "        # Global measures\n",
    "        #avg_shrt = igraph.average_path_length()\n",
    "        global_efficiency = calc_global_efficiency(network) # How efficiently information is exchanged (higher values indicate higher integration) also high segregation?\n",
    "        \n",
    "        # Append data to the list\n",
    "        data.append({\n",
    "            \"subject\": graph_info[\"subject\"],\n",
    "            \"group\": graph_info[\"group\"],\n",
    "            \"timepoint\": graph_info[\"timepoint\"],\n",
    "            \"condition\": graph_info[\"condition\"],\n",
    "            \"g_ef\": global_efficiency\n",
    "            })\n",
    "   \n",
    "    # Create a DataFrame from the list of data\n",
    "    global_df = pd.DataFrame(data)\n",
    "\n",
    "    return global_df\n",
    "\n",
    "# Local Network Measures\n",
    "def calculate_local_network_measures(all_graphs, electrode_names):\n",
    "    # Create an empty list to store data\n",
    "    clustering_temp = []\n",
    "    betweenness_temp = []\n",
    "    indegree_temp = []\n",
    "    outdegree_temp = []\n",
    "\n",
    "    # Loop through each loaded graph\n",
    "    for graph_info in all_graphs:\n",
    "        subject = graph_info[\"subject\"]\n",
    "        response_category = graph_info[\"group\"]\n",
    "        timepoint = graph_info[\"timepoint\"]\n",
    "        task = graph_info[\"condition\"]\n",
    "        network = graph_info[\"networkx\"]\n",
    "\n",
    "        # Local measures\n",
    "        clustering = nx.clustering(network)\n",
    "        betweenness = nx.betweenness_centrality(network)\n",
    "        indegree = network.in_degree() \n",
    "        outdegree = network.out_degree()\n",
    "        \n",
    "        # Rename columns with electrode names (replace this with your electrode names)\n",
    "        clustering_renamed = {electrode_names[i]: clustering.get(i, 0) for i in range(len(electrode_names))}\n",
    "        betweenness_renamed = {electrode_names[i]: betweenness.get(i, 0) for i in range(len(electrode_names))}\n",
    "        indegree_renamed = {electrode_names[i]: indegree[i] for i in range(len(electrode_names))}\n",
    "        outdegree_renamed = {electrode_names[i]: outdegree[i] for i in range(len(electrode_names))}\n",
    "\n",
    "        # Append data to the lists\n",
    "        clustering_temp.append({\n",
    "            \"subject\": subject,\n",
    "            \"group\": response_category,\n",
    "            \"timepoint\": timepoint,\n",
    "            \"condition\": task,\n",
    "            \"measure\": \"ClCoef\",\n",
    "            ** clustering_renamed\n",
    "            }) \n",
    "         \n",
    "        betweenness_temp.append({\n",
    "            \"subject\": subject,\n",
    "            \"group\": response_category,\n",
    "            \"timepoint\": timepoint,\n",
    "            \"condition\": task,\n",
    "            \"measure\": \"Btwn\",\n",
    "            ** betweenness_renamed\n",
    "            })  \n",
    "        \n",
    "        indegree_temp.append({\n",
    "            \"subject\": subject,\n",
    "            \"group\": response_category,\n",
    "            \"timepoint\": timepoint,\n",
    "            \"condition\": task,\n",
    "            \"measure\": \"InDgr\",\n",
    "            ** indegree_renamed\n",
    "            })  \n",
    "        \n",
    "        outdegree_temp.append({\n",
    "            \"subject\": subject,\n",
    "            \"group\": response_category,\n",
    "            \"timepoint\": timepoint,\n",
    "            \"condition\": task,\n",
    "            \"measure\": \"OutDgr\",\n",
    "            ** outdegree_renamed\n",
    "            })  \n",
    "        \n",
    "    # Create a DataFrame from the list of data\n",
    "    clustering_df = pd.DataFrame(clustering_temp)\n",
    "    betweenness_df = pd.DataFrame(betweenness_temp)\n",
    "    indegree_df = pd.DataFrame(indegree_temp)\n",
    "    outdegree_df = pd.DataFrame(outdegree_temp)\n",
    "\n",
    "    # Combine dataframes\n",
    "    local_df = pd.concat([clustering_df, betweenness_df, indegree_df, outdegree_df], axis=0)\n",
    "\n",
    "    # Sort the DataFrame by the index\n",
    "    local_df.sort_index(inplace=True)\n",
    "\n",
    "    return local_df \n",
    "\n",
    "def te_extraction (all_graphs):\n",
    "\n",
    "    # Initialize temporary variables\n",
    "    subject_values = []\n",
    "    task_values = []\n",
    "    timepoint_values = []\n",
    "    response_values = []\n",
    "\n",
    "    fp_asymmetry_left_values = []\n",
    "    fp_asymmetry_right_values = []\n",
    "    ft_asymmetry_left_values = []\n",
    "    ft_asymmetry_right_values = []\n",
    "    fo_asymmetry_left_values = []\n",
    "    fo_asymmetry_right_values = []\n",
    "\n",
    "    tp_asymmetry_left_values = []\n",
    "    tp_asymmetry_right_values = []\n",
    "    to_asymmetry_left_values = []\n",
    "    to_asymmetry_right_values = []\n",
    "\n",
    "    po_asymmetry_left_values = []\n",
    "    po_asymmetry_right_values = []\n",
    "\n",
    "    # Iterate through each participant-file in the list\n",
    "    for participant in range(len(all_graphs)):\n",
    "\n",
    "        # Load participant details\n",
    "        subject = all_graphs[participant]['subject']\n",
    "        task = all_graphs[participant]['condition']\n",
    "        timepoint = all_graphs[participant]['timepoint']\n",
    "        response_category = all_graphs[participant]['group']\n",
    "\n",
    "        # Load the channel weights list per participant-file\n",
    "        channel_weights_list = all_graphs[participant]['adj_matrix'].get_edge_list() # where the weights are the te values\n",
    "\n",
    "        # Initialize sums\n",
    "        fp_caudal_left_sum = 0\n",
    "        fp_caudal_right_sum = 0\n",
    "        fp_rostral_left_sum = 0\n",
    "        fp_rostral_right_sum = 0\n",
    "\n",
    "        # Iterate through the list and apply the conditional statement\n",
    "        for i, j, weight in channel_weights_list:\n",
    "            \n",
    "            # Left hemisphere frontal to parietal: Fp1, AF3, F7, F3, Fz --> FC1, C3, CP1, P3, Pz, Cz\n",
    "            if (i in [0, 1, 2, 3, 30]) and (j in [4, 7, 8, 11, 12, 31]):\n",
    "                fp_caudal_left_sum += weight\n",
    "\n",
    "            # Right hemisphere frontal to parietal: Fp2, AF4, F8, F4, Fz, Pz --> FC2, C4, CP2, P4, Pz, Cz\n",
    "            if (i in [29, 28, 27, 26, 30]) and (j in [25, 22, 21, 18, 12, 31]):\n",
    "                fp_caudal_right_sum += weight\n",
    "\n",
    "            # Left hemisphere parietal to frontal: FC1, C3, CP1, P3, Pz, Cz --> Fp1, AF3, F7, F3, Fz\n",
    "            if (i in [4, 7, 8, 11, 12, 31]) and (j in [0, 1, 2, 3, 30]):\n",
    "                fp_rostral_left_sum += weight\n",
    "\n",
    "            # Right hemisphere parietal to frontal: FC2, C4, CP2, P4, Pz, Cz --> Fp2, AF4, F8, F4, Fz\n",
    "            if (i in [25, 22, 21, 18, 12, 31]) and (j in [29, 28, 27, 26, 30]):\n",
    "                fp_rostral_right_sum += weight\n",
    "\n",
    "        # Calculate the final values by scaling by the 1/number of node combinations\n",
    "        fp_caudal_left_final_value = fp_caudal_left_sum * (1/30)\n",
    "        fp_caudal_right_final_value = fp_caudal_right_sum * (1/30)\n",
    "        fp_rostral_left_final_value = fp_rostral_left_sum * (1/30)\n",
    "        fp_rostral_right_final_value = fp_rostral_right_sum * (1/30)\n",
    "\n",
    "        # Calculate asymmetry values for frontal-parietal\n",
    "        if (fp_caudal_left_final_value + fp_rostral_left_final_value) == 0:\n",
    "            fp_asymmetry_left = 0.5\n",
    "        else:\n",
    "            fp_asymmetry_left = (fp_caudal_left_final_value - fp_rostral_left_final_value) / (fp_caudal_left_final_value + fp_rostral_left_final_value)\n",
    "\n",
    "            # if you want to transform to 0-1 scale\n",
    "            # fp_asymmetry_left_transformed = (fp_asymmetry_left + 1) / 2\n",
    "            # fp_asymmetry_left_transformed = round(fp_asymmetry_left_transformed, 3)  # Round to 3 decimal places\n",
    "\n",
    "        if (fp_caudal_right_final_value + fp_rostral_right_final_value) == 0:\n",
    "            fp_asymmetry_right = 0.5\n",
    "        else:\n",
    "            fp_asymmetry_right = (fp_caudal_right_final_value - fp_rostral_right_final_value) / (fp_caudal_right_final_value + fp_rostral_right_final_value)\n",
    "            # fp_asymmetry_right_transformed = (fp_asymmetry_right + 1) / 2\n",
    "            # fp_asymmetry_right_transformed = round(fp_asymmetry_right_transformed, 3)  # Round to 3 decimal places\n",
    "        \n",
    "        # Append asymmetry values as needed\n",
    "        fp_asymmetry_left_values.append(fp_asymmetry_left)\n",
    "        fp_asymmetry_right_values.append(fp_asymmetry_right)\n",
    "\n",
    "        # Initialize sums\n",
    "        ft_caudal_left_sum = 0\n",
    "        ft_caudal_right_sum = 0\n",
    "        ft_rostral_left_sum = 0\n",
    "        ft_rostral_right_sum = 0\n",
    "\n",
    "        # Iterate through the list and apply the conditional statement\n",
    "        for i, j, weight in channel_weights_list:\n",
    "            \n",
    "            # Left hemisphere frontal to temporal: Fp1, AF3, F7, F3, Fz --> FC5, T7, CP5, P7\n",
    "            if (i in [0, 1, 2, 3, 30]) and (j in [5, 6, 9, 10]):\n",
    "                ft_caudal_left_sum += weight\n",
    "\n",
    "            # Right hemisphere frontal to temporal: Fp2, AF4, F8, F4, Fz --> P8, CP6, T8, FC6\n",
    "            if (i in [29, 28, 27, 26, 30]) and (j in [24, 23, 20, 19]):\n",
    "                ft_caudal_right_sum += weight\n",
    "\n",
    "            # Left hemisphere temporal to frontal: FC5, T7, CP5, P7 --> Fp1, AF3, F7, F3, Fz\n",
    "            if (i in [5, 6, 9, 10]) and (j in [0, 1, 2, 3, 30]):\n",
    "                ft_rostral_left_sum += weight\n",
    "\n",
    "            # Right hemisphere temporal to frontal: P8, CP6, T8, FC6 --> Fp2, AF4, F8, F4, Fz\n",
    "            if (i in [24, 23, 20, 19]) and (j in [29, 28, 27, 26, 30]):\n",
    "                ft_rostral_right_sum += weight\n",
    "\n",
    "        # Calculate the final values by scaling by the 1/number of pairs\n",
    "        ft_caudal_left_final_value = ft_caudal_left_sum * (1/20)\n",
    "        ft_caudal_right_final_value = ft_caudal_right_sum * (1/20)\n",
    "        ft_rostral_left_final_value = ft_rostral_left_sum * (1/20)\n",
    "        ft_rostral_right_final_value = ft_rostral_right_sum * (1/20)\n",
    "\n",
    "        # Calculate asymmetry values for fronto-temporal\n",
    "        if (ft_caudal_left_final_value + ft_rostral_left_final_value) == 0:\n",
    "            ft_asymmetry_left = 0.5\n",
    "        else:\n",
    "            ft_asymmetry_left = (ft_caudal_left_final_value - ft_rostral_left_final_value) / (ft_caudal_left_final_value + ft_rostral_left_final_value)\n",
    "\n",
    "            # if you want to transform to 0-1 scale\n",
    "            # ft_asymmetry_left_transformed = (ft_asymmetry_left + 1) / 2\n",
    "            # ft_asymmetry_left_transformed = round(ft_asymmetry_left_transformed, 3)  # Round to 3 decimal places\n",
    "\n",
    "        if (ft_caudal_right_final_value + ft_rostral_right_final_value) == 0:\n",
    "            ft_asymmetry_right = 0.5\n",
    "        else:\n",
    "            ft_asymmetry_right = (ft_caudal_right_final_value - ft_rostral_right_final_value) / (ft_caudal_right_final_value + ft_rostral_right_final_value)\n",
    "\n",
    "            # if you want to transform to 0-1 scale\n",
    "            # ft_asymmetry_right_transformed = (ft_asymmetry_right + 1) / 2\n",
    "            # ft_asymmetry_right_transformed = round(ft_asymmetry_right_transformed, 3)  # Round to 3 decimal places\n",
    "\n",
    "        # Append asymmetry values as needed\n",
    "        ft_asymmetry_left_values.append(ft_asymmetry_left)\n",
    "        ft_asymmetry_right_values.append(ft_asymmetry_right)\n",
    "\n",
    "        # Initialize sums\n",
    "        fo_caudal_left_sum = 0\n",
    "        fo_caudal_right_sum = 0\n",
    "        fo_rostral_left_sum = 0\n",
    "        fo_rostral_right_sum = 0\n",
    "\n",
    "        # Iterate through the list and apply the conditional statement\n",
    "        for i, j, weight in channel_weights_list:\n",
    "            \n",
    "            # Left hemisphere frontal to occipital: Fp1, AF3, F7, F3, Fz --> PO3, O1, Oz\n",
    "            if (i in [0, 1, 2, 3, 30]) and (j in [13, 14, 15]):\n",
    "                fo_caudal_left_sum += weight\n",
    "\n",
    "            # Right hemisphere frontal to occipital: Fp2, AF4, F8, F4, Fz --> PO4, O2, Oz\n",
    "            if (i in [29, 28, 27, 26, 30]) and (j in [17, 16, 15]):\n",
    "                fo_caudal_right_sum += weight\n",
    "\n",
    "            # Left hemisphere occipital to frontal: PO3, O1, Oz --> Fp1, AF3, F7, F3, Fz\n",
    "            if (i in [13, 14, 15]) and (j in [0, 1, 2, 3, 30]):\n",
    "                fo_rostral_left_sum += weight\n",
    "\n",
    "            # Right hemisphere occipital to frontal: PO4, O2, Oz --> Fp2, AF4, F8, F4, Fz\n",
    "            if (i in [17, 16, 15]) and (j in [29, 28, 27, 26, 30]):\n",
    "                fo_rostral_right_sum += weight\n",
    "\n",
    "        # Calculate the final values by scaling by the 1/number of pairs\n",
    "        fo_caudal_left_final_value = fo_caudal_left_sum * (1/15)\n",
    "        fo_caudal_right_final_value = fo_caudal_right_sum * (1/15)\n",
    "        fo_rostral_left_final_value = fo_rostral_left_sum * (1/15)\n",
    "        fo_rostral_right_final_value = fo_rostral_right_sum * (1/15)\n",
    "\n",
    "        # Calculate asymmetry values for fronto-occipital\n",
    "        if (fo_caudal_left_final_value + fo_rostral_left_final_value) == 0:\n",
    "            fo_asymmetry_left = 0.5\n",
    "        else:\n",
    "            fo_asymmetry_left = (fo_caudal_left_final_value - fo_rostral_left_final_value) / (fo_caudal_left_final_value + fo_rostral_left_final_value)\n",
    "\n",
    "            # if you want to transform to 0-1 scale\n",
    "            # fo_asymmetry_left_transformed = (fo_asymmetry_left + 1) / 2\n",
    "            # fo_asymmetry_left_transformed = round(fo_asymmetry_left_transformed, 3)  # Round to 3 decimal places\n",
    "\n",
    "        if (fo_caudal_right_final_value + fo_rostral_right_final_value) == 0:\n",
    "            fo_asymmetry_right = 0.5\n",
    "        else:\n",
    "            fo_asymmetry_right = (fo_caudal_right_final_value - fo_rostral_right_final_value) / (fo_caudal_right_final_value + fo_rostral_right_final_value)\n",
    "\n",
    "            # if you want to transform to 0-1 scale\n",
    "            # fo_asymmetry_right_transformed = (fo_asymmetry_right + 1) / 2\n",
    "            # fo_asymmetry_right_transformed = round(fo_asymmetry_right_transformed, 3)  # Round to 3 decimal places\n",
    "\n",
    "        # Append asymmetry values as needed\n",
    "        fo_asymmetry_left_values.append(fo_asymmetry_left)\n",
    "        fo_asymmetry_right_values.append(fo_asymmetry_right)\n",
    "\n",
    "        # Initialize sums\n",
    "        tp_caudal_left_sum = 0\n",
    "        tp_caudal_right_sum = 0\n",
    "        tp_rostral_left_sum = 0\n",
    "        tp_rostral_right_sum = 0\n",
    "\n",
    "        # Iterate through the list and apply the conditional statement\n",
    "        for i, j, weight in channel_weights_list:\n",
    "            \n",
    "            # Left hemisphere temporal to parietal: FC5, T7, CP5, P7 --> FC1, C3, CP1, P3, Pz, Cz\n",
    "            if (i in [5, 6, 9, 10]) and (j in [4, 7, 8, 11, 12, 31]):\n",
    "                tp_caudal_left_sum += weight\n",
    "\n",
    "            # Right hemisphere temporal to parietal: P8, CP6, T8, FC6 --> FC2, C4, CP2, P4, Pz, Cz\n",
    "            if (i in [24, 23, 20, 19]) and (j in [25, 22, 21, 18, 12, 31]):\n",
    "                tp_caudal_right_sum += weight\n",
    "\n",
    "            # Left hemisphere parietal to temporal: FC1, C3, CP1, P3, Pz, Cz --> FC5, T7, CP5, P7\n",
    "            if (i in [4, 7, 8, 11, 12, 31]) and (j in [5, 6, 9, 10]):\n",
    "                tp_rostral_left_sum += weight\n",
    "\n",
    "            # Right hemisphere parietal to temporal: FC2, C4, CP2, P4, Pz, Cz --> P8, CP6, T8, FC6\n",
    "            if (i in [25, 22, 21, 18, 12, 31]) and (j in [24, 23, 20, 19]):\n",
    "                tp_rostral_right_sum += weight\n",
    "\n",
    "        # Calculate the final values by scaling by the 1/number of pairs\n",
    "        tp_caudal_left_final_value = tp_caudal_left_sum * (1/24)\n",
    "        tp_caudal_right_final_value = tp_caudal_right_sum * (1/24)\n",
    "        tp_rostral_left_final_value = tp_rostral_left_sum * (1/24)\n",
    "        tp_rostral_right_final_value = tp_rostral_right_sum * (1/24)\n",
    "\n",
    "        # Calculate asymmetry values for temporal-parietal\n",
    "        if (tp_caudal_left_final_value + tp_rostral_left_final_value) == 0:\n",
    "            tp_asymmetry_left = 0.5\n",
    "        else:\n",
    "            tp_asymmetry_left = (tp_caudal_left_final_value - tp_rostral_left_final_value) / (tp_caudal_left_final_value + tp_rostral_left_final_value)\n",
    "\n",
    "            # if you want to transform to 0-1 scale\n",
    "            # tp_asymmetry_left_transformed = (tp_asymmetry_left + 1) / 2\n",
    "            # tp_asymmetry_left_transformed = round(tp_asymmetry_left_transformed, 3)  # Round to 3 decimal places\n",
    "\n",
    "        if (tp_caudal_right_final_value + tp_rostral_right_final_value) == 0:\n",
    "            tp_asymmetry_right = 0.5\n",
    "        else:\n",
    "            tp_asymmetry_right = (tp_caudal_right_final_value - tp_rostral_right_final_value) / (tp_caudal_right_final_value + tp_rostral_right_final_value)\n",
    "\n",
    "            # if you want to transform to 0-1 scale\n",
    "            # tp_asymmetry_right_transformed = (tp_asymmetry_right + 1) / 2\n",
    "            # tp_asymmetry_right_transformed = round(tp_asymmetry_right_transformed, 3)  # Round to 3 decimal places\n",
    "\n",
    "        # Print or return the asymmetry values as needed\n",
    "        tp_asymmetry_left_values.append(tp_asymmetry_left)\n",
    "        tp_asymmetry_right_values.append(tp_asymmetry_right)\n",
    "\n",
    "        # Initialize sums\n",
    "        to_caudal_left_sum = 0\n",
    "        to_caudal_right_sum = 0\n",
    "        to_rostral_left_sum = 0\n",
    "        to_rostral_right_sum = 0\n",
    "\n",
    "        # Iterate through the list and apply the conditional statement\n",
    "        for i, j, weight in channel_weights_list:\n",
    "            \n",
    "            # Left hemisphere temporal to occipital: T7, CP5, P7, FC5 --> PO3, O1, Oz\n",
    "            if (i in [5, 6, 9, 10]) and (j in [13, 14, 15]):\n",
    "                to_caudal_left_sum += weight\n",
    "\n",
    "            # Right hemisphere temporal to occipital: P8, CP6, T8, FC6 --> PO4, O2, Oz\n",
    "            if (i in [24, 23, 20, 19]) and (j in [17, 16, 15]):\n",
    "                to_caudal_right_sum += weight\n",
    "\n",
    "            # Left hemisphere occipital to temporal: PO3, O1, Oz --> T7, CP5, P7, FC5\n",
    "            if (i in [13, 14, 15]) and (j in [5, 6, 9, 10]):\n",
    "                to_rostral_left_sum += weight\n",
    "\n",
    "            # Right hemisphere occipital to temporal: PO4, O2, Oz --> P8, CP6, T8, FC6\n",
    "            if (i in [17, 16, 15]) and (j in [24, 23, 20, 19]):\n",
    "                to_rostral_right_sum += weight\n",
    "\n",
    "        # Calculate the final values by scaling by the 1/number of pairs\n",
    "        to_caudal_left_final_value = to_caudal_left_sum * (1/12)\n",
    "        to_caudal_right_final_value = to_caudal_right_sum * (1/12)\n",
    "        to_rostral_left_final_value = to_rostral_left_sum * (1/12)\n",
    "        to_rostral_right_final_value = to_rostral_right_sum * (1/12)\n",
    "\n",
    "        # Calculate asymmetry values for temporal-occipital\n",
    "        if (to_caudal_left_final_value + to_rostral_left_final_value) == 0:\n",
    "            to_asymmetry_left = 0.5\n",
    "        else:\n",
    "            to_asymmetry_left = (to_caudal_left_final_value - to_rostral_left_final_value) / (to_caudal_left_final_value + to_rostral_left_final_value)\n",
    "\n",
    "            # if you want to transform to 0-1 scale\n",
    "            # to_asymmetry_left_transformed = (to_asymmetry_left + 1) / 2\n",
    "            # to_asymmetry_left_transformed = round(to_asymmetry_left_transformed, 3)  # Round to 3 decimal places\n",
    "\n",
    "        if (to_caudal_right_final_value + to_rostral_right_final_value) == 0:\n",
    "            to_asymmetry_right = 0.5\n",
    "        else:\n",
    "            to_asymmetry_right = (to_caudal_right_final_value - to_rostral_right_final_value) / (to_caudal_right_final_value + to_rostral_right_final_value)\n",
    "\n",
    "            # if you want to transform to 0-1 scale\n",
    "            # to_asymmetry_right_transformed = (to_asymmetry_right + 1) / 2\n",
    "            # to_asymmetry_right_transformed = round(to_asymmetry_right_transformed, 3)  # Round to 3 decimal places\n",
    "\n",
    "        # Print or return the asymmetry values as needed\n",
    "        to_asymmetry_left_values.append(to_asymmetry_left)\n",
    "        to_asymmetry_right_values.append(to_asymmetry_right)\n",
    "\n",
    "        # Initialize sums\n",
    "        po_caudal_left_sum = 0\n",
    "        po_caudal_right_sum = 0\n",
    "        po_rostral_left_sum = 0\n",
    "        po_rostral_right_sum = 0\n",
    "\n",
    "        # Iterate through the list and apply the conditional statement\n",
    "        for i, j, weight in channel_weights_list:\n",
    "            \n",
    "            # Left hemisphere parietal to occipital: FC1, C3, CP1, P3, Pz, Cz --> PO3, O1, Oz\n",
    "            if (i in [4, 7, 8, 11, 12, 31]) and (j in [13, 14, 15]):\n",
    "                po_caudal_left_sum += weight\n",
    "\n",
    "            # Right hemisphere parietal to occipital: FC2, C4, CP2, P4, Pz, Cz --> PO4, O2, Oz\n",
    "            if (i in [25, 22, 21, 18, 12, 31]) and (j in [17, 16, 15]):\n",
    "                po_caudal_right_sum += weight\n",
    "\n",
    "            # Left hemisphere occipital to parietal: PO3, O1, Oz --> FC1, C3, CP1, P3, Pz, Cz\n",
    "            if (i in [13, 14, 15]) and (j in [4, 7, 8, 11, 12, 31]):\n",
    "                po_rostral_left_sum += weight\n",
    "\n",
    "            # Right hemisphere occipital to parietal: PO4, O2, Oz --> FC2, C4, CP2, P4, Pz, Cz\n",
    "            if (i in [17, 16, 15]) and (j in [25, 22, 21, 18, 12, 31]):\n",
    "                po_rostral_right_sum += weight\n",
    "\n",
    "        # Calculate the final values by scaling by the 1/number of pairs\n",
    "        po_caudal_left_final_value = po_caudal_left_sum * (1/18)\n",
    "        po_caudal_right_final_value = po_caudal_right_sum * (1/18)\n",
    "        po_rostral_left_final_value = po_rostral_left_sum * (1/18)\n",
    "        po_rostral_right_final_value = po_rostral_right_sum * (1/18)\n",
    "\n",
    "        # Calculate asymmetry values for parieto-occipital\n",
    "        if (po_caudal_left_final_value + po_rostral_left_final_value) == 0:\n",
    "            po_asymmetry_left = 0.5\n",
    "        else:\n",
    "            po_asymmetry_left = (po_caudal_left_final_value - po_rostral_left_final_value) / (po_caudal_left_final_value + po_rostral_left_final_value)\n",
    "\n",
    "            # if you want to transform to 0-1 scale\n",
    "            # po_asymmetry_left_transformed = (po_asymmetry_left + 1) / 2\n",
    "            # po_asymmetry_left_transformed = round(po_asymmetry_left_transformed, 3)  # Round to 3 decimal places\n",
    "\n",
    "        if (po_caudal_right_final_value + po_rostral_right_final_value) == 0:\n",
    "            po_asymmetry_right = 0.5\n",
    "        else:\n",
    "            po_asymmetry_right = (po_caudal_right_final_value - po_rostral_right_final_value) / (po_caudal_right_final_value + po_rostral_right_final_value)\n",
    "\n",
    "            # if you want to transform to 0-1 scale\n",
    "            # po_asymmetry_right_transformed = (po_asymmetry_right + 1) / 2\n",
    "            # po_asymmetry_right_transformed = round(po_asymmetry_right_transformed, 3)  # Round to 3 decimal places\n",
    "\n",
    "        # Print or return the asymmetry values as needed\n",
    "        po_asymmetry_left_values.append(po_asymmetry_left)\n",
    "        po_asymmetry_right_values.append(po_asymmetry_right)\n",
    "\n",
    "        # # Append identifiers\n",
    "        subject_values.append(subject)\n",
    "        task_values.append(task)\n",
    "        timepoint_values.append(timepoint)\n",
    "        response_values.append(response_category)\n",
    "\n",
    "        # Save the results to a DataFrame with an ID column\n",
    "        te_extracted_df = pd.DataFrame({\n",
    "            'subject': subject_values,\n",
    "            'group': response_values,\n",
    "            'condition': task_values,\n",
    "            'timepoint': timepoint_values, # add subject ID, task, timepoint\n",
    "            'FP_Asymmetry_Left': fp_asymmetry_left_values,\n",
    "            'FP_Asymmetry_Right': fp_asymmetry_right_values,\n",
    "            'FT_Asymmetry_Left': ft_asymmetry_left_values,\n",
    "            'FT_Asymmetry_Right': ft_asymmetry_right_values,            \n",
    "            'FO_Asymmetry_Left': fo_asymmetry_left_values,\n",
    "            'FO_Asymmetry_Right': fo_asymmetry_right_values,\n",
    "            'TP_Asymmetry_Left': tp_asymmetry_left_values,\n",
    "            'TP_Asymmetry_Right': tp_asymmetry_right_values,\n",
    "            'TO_Asymmetry_Left': to_asymmetry_left_values,\n",
    "            'TO_Asymmetry_Right': to_asymmetry_right_values,\n",
    "            'PO_Asymmetry_Left': po_asymmetry_left_values,\n",
    "            'PO_Asymmetry_Right': po_asymmetry_right_values\n",
    "            })\n",
    "\n",
    "    return te_extracted_df  \n",
    "\n",
    "def questionnaire_combine(core_data, questionnare_data, processing_data):\n",
    "\n",
    "\t# Rename columns and select a sub-set\n",
    "\tquestionnare_data.rename(columns={\"base_id\": \"Subject\", 'age': 'Age', 'gender_cat':\"Sex\", \"BSS_score.00A\": \"BSS_ses01\", \"BSS_score.06D\": \"BSS_ses02\", \n",
    "\t\t\t\t\t\t\t\t\t  \"BSS_score.07A\":\"BSS_ses03\", \"MADRS_score.00A\": \"MADRS_ses01\", \"MADRS_score.07A\": \"MADRS_ses03\", 'DASS_Dep.00A': 'DASSdep_ses01', \n",
    "                                      'DASS_Dep.06D': 'DASSdep_ses02', 'DASS_Dep.07A': 'DASSdep_ses03', 'DASS_Anx.00A': 'DASSanx_ses01', \n",
    "                                      'DASS_Anx.06D': 'DASSanx_ses02', 'Dass21_anxiety.07A': 'DASSanx_ses03', 'DASS_Stress.00A': 'DASSstress_ses01', \n",
    "                                      'DASS_Stress.06D': 'DASSstress_ses02', 'Dass21_stress.07A': 'DASSstress_ses03', \n",
    "\t\t\t\t\t\t\t\t\t  \"BSS_Responder_cat\": \"Response_Post\", \"BSS_Responder_FUP_cat\": \"Response_FUP\"}, inplace = True)\n",
    "\t\n",
    "\tquestionnare_data = questionnare_data[['Subject', 'Sex', 'Age','BSS_ses01', 'BSS_ses02', 'BSS_ses03', 'MADRS_ses01', 'MADRS_ses03', \n",
    "                                        'DASSdep_ses01', 'DASSdep_ses02', 'DASSdep_ses03','DASSanx_ses01', 'DASSanx_ses02', 'DASSanx_ses03',\n",
    "                                        'DASSstress_ses01', 'DASSstress_ses02', 'DASSstress_ses03','Response_Post', 'Response_FUP']]\n",
    "\t\n",
    "\t# Align subject ID's for combining.\n",
    "\tquestionnare_data['Subject'].replace('OKTOS_00', 'sub-', regex=True, inplace=True)\n",
    "\n",
    "\t# Loop through each subject and BSS timepoint value in qualtrics df and add to the corresponding subject/Tx in the master\n",
    "\tfor s, bss_value in zip(questionnare_data['Subject'], questionnare_data['BSS_ses01']):\n",
    "\t\tcondition = (core_data['subject'] == s) & (core_data['timepoint'] == 'ses-01')\n",
    "\t\tcore_data.loc[condition, 'BSS'] = bss_value\n",
    "\n",
    "\tfor s, bss_value in zip(questionnare_data['Subject'], questionnare_data['BSS_ses02']):\n",
    "\t\tcondition = (core_data['subject'] == s) & (core_data['timepoint'] == 'ses-02')\n",
    "\t\tcore_data.loc[condition, 'BSS'] = bss_value\n",
    "\n",
    "\tfor s, bss_value in zip(questionnare_data['Subject'], questionnare_data['BSS_ses03']):\n",
    "\t\tcondition = (core_data['subject'] == s) & (core_data['timepoint'] == 'ses-03')\n",
    "\t\tcore_data.loc[condition, 'BSS'] = bss_value\n",
    "\n",
    "\tfor s, madrs_value in zip(questionnare_data['Subject'], questionnare_data['MADRS_ses01']):\n",
    "\t\tcondition = (core_data['subject'] == s) & (core_data['timepoint'] == 'ses-01')\n",
    "\t\tcore_data.loc[condition, 'MADRS'] = madrs_value\n",
    "\n",
    "\tfor s, madrs_value in zip(questionnare_data['Subject'], questionnare_data['MADRS_ses03']):\n",
    "\t\tcondition = (core_data['subject'] == s) & (core_data['timepoint'] == 'ses-03')\n",
    "\t\tcore_data.loc[condition, 'MADRS'] = madrs_value\n",
    "\n",
    "\tfor s, sex in zip(questionnare_data['Subject'], questionnare_data['Sex']):\n",
    "\t\tcondition = (core_data['subject'] == s)\n",
    "\t\tcore_data.loc[condition, 'Sex'] = sex\n",
    "\n",
    "\tfor s, age in zip(questionnare_data['Subject'], questionnare_data['Age']):\n",
    "\t\tcondition = (core_data['subject'] == s)\n",
    "\t\tcore_data.loc[condition, 'Age'] = age\n",
    "\n",
    "\tfor s, time in zip(processing_data['subj_ID'], processing_data['tx']):\n",
    "\t\tcondition = (core_data['subject'] == s) & (core_data['timepoint'] == time)\n",
    "\t\tcore_data.loc[condition, 'is_before_12'] = processing_data['is_before_12']\n",
    "\n",
    "\tcore_data['MADRS'].replace(0, 999,inplace=True)\n",
    "\n",
    "\treturn core_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory and file paths using pathlib\n",
    "root_dir = Path(\"C:/Users/j_m289/Pictures/phd/3. Data Analysis/studies/OKTOS/resting_te\")\n",
    "data_folder = root_dir / \"data/derivatives\" \n",
    "analysis_folder = root_dir / \"analysis\"\n",
    "response_file = root_dir / 'response_status_mapping.txt'\n",
    "electrode_file = root_dir / 'electrode_names.txt'\n",
    "questionnaire_data = analysis_folder / 'spreadsheets' / 'OKTOS_Qualtrics_master.csv'\n",
    "processing_data = analysis_folder / 'spreadsheets' / 'Recording_parameters.csv'\n",
    "\n",
    "# Load response status classification file\n",
    "response_mapping = load_classifier_file(response_file)\n",
    "\n",
    "# Load electrode names (used for converting source-target pairs from IDTXL to channel names)\n",
    "electrode_names = load_electrodes_file(electrode_file)\n",
    "electrode_dict = {i: electrode for i, electrode in enumerate(electrode_names)}\n",
    "\n",
    "# Load relevant data to combine with extracted metrics\n",
    "qualtrics = pd.read_csv(questionnaire_data)\n",
    "eeg_scantime = pd.read_csv(processing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Transfer Entropy Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files (Note, there is currently an issue with the gml file load (something to do with strings))\n",
    "all_graphs = load_graphs(data_folder, weights='binary')\n",
    "\n",
    "# Add response status info\n",
    "add_classifier(all_graphs, response_mapping, designation=[1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TE Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TE values for significant source-target pairs and calculate information flow metrics\n",
    "te_data = te_extraction(all_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topographical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global measures\n",
    "global_data = calculate_global_network_measures(all_graphs)\n",
    "\n",
    "# Local measures\n",
    "local_data = calculate_local_network_measures(all_graphs, electrode_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe re-arrangement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrames to reshape so that regions/channels labels are in one column and the metrics are in another. \n",
    "te_data_long = pd.melt(te_data, id_vars=['subject', 'group', 'condition', 'timepoint'], \n",
    "                  value_vars=['FP_Asymmetry_Left', 'FP_Asymmetry_Right', \n",
    "                              'FT_Asymmetry_Left', 'FT_Asymmetry_Right',\n",
    "                              'FO_Asymmetry_Left', 'FO_Asymmetry_Right',\n",
    "                              'TP_Asymmetry_Left', 'TP_Asymmetry_Right',\n",
    "                              'TO_Asymmetry_Left', 'TO_Asymmetry_Right',\n",
    "                              'PO_Asymmetry_Left', 'PO_Asymmetry_Right'],\n",
    "                  var_name='regions', value_name='asymmetry')\n",
    "\n",
    "# Clean the 'regions' column to remove the 'Asymmetry' and 'Asymmetry_Right' parts\n",
    "te_data_long['regions'] = te_data_long['regions'].str.replace(r'_Asymmetry', '', regex=True)\n",
    "\n",
    "# Melt the DataFrame to reshape the columns starting from 'FP_Asymmetry_Left'\n",
    "local_data_long = pd.melt(local_data, id_vars=['subject', 'group', 'condition', 'timepoint', 'measure'], \n",
    "                  value_vars=electrode_names,\n",
    "                  var_name='channels', value_name='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regions for local_data\n",
    "regions_dict = {\n",
    "    'Frontal': {\n",
    "        'Left': ['Fp1', 'AF3', 'F7', 'F3'],\n",
    "        'Right': ['Fp2', 'AF4', 'F8', 'F4'],\n",
    "        'Midline': ['Fz']\n",
    "    },\n",
    "    'Central': {\n",
    "        'Midline': ['Cz']\n",
    "    },\n",
    "    'Parietal': {\n",
    "        'Left': ['FC1', 'C3', 'CP1', 'P3'],\n",
    "        'Right': ['FC2', 'C4', 'CP2', 'P4'],\n",
    "        'Midline': ['Pz']\n",
    "    },\n",
    "    'Temporal': {\n",
    "        'Left': ['FC5', 'T7', 'CP5', 'P7'],\n",
    "        'Right': ['P8', 'CP6', 'T8', 'FC6']\n",
    "    },\n",
    "    'Occipital': {\n",
    "        'Left': ['PO3', 'O1'],\n",
    "        'Right': ['PO4', 'O2'],\n",
    "        'Midline': ['Oz']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to get the region for a given channel\n",
    "def get_region(channel):\n",
    "    for region, sides in regions_dict.items():\n",
    "        for side, channels in sides.items():\n",
    "            if channel in channels:\n",
    "                return f\"{region}_{side}\"\n",
    "    return None  # Return None if channel is not found\n",
    "\n",
    "# Apply the function to create the 'region' column in the local_data_long\n",
    "local_data_long['region'] = local_data_long['channels'].apply(get_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Concatenation and Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_data_long = questionnaire_combine(core_data=te_data_long, questionnare_data=qualtrics, processing_data=eeg_scantime)\n",
    "global_data_long = questionnaire_combine(core_data=global_data, questionnare_data=qualtrics, processing_data=eeg_scantime)\n",
    "local_data_long = questionnaire_combine(core_data=local_data_long, questionnare_data=qualtrics, processing_data=eeg_scantime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_data_long.to_csv(analysis_folder/'spreadsheets/OKTOS_te_asymmetry.csv', index=False)\n",
    "global_data_long.to_csv(analysis_folder/'spreadsheets/OKTOS_te_global.csv', index=False)\n",
    "local_data_long.to_csv(analysis_folder/'spreadsheets/OKTOS_te_local.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idtxl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
